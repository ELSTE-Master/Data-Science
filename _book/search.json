[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ELSTE Data Science Master Course",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "class1/class1_overview.html",
    "href": "class1/class1_overview.html",
    "title": "Class 1",
    "section": "",
    "text": "Today’s objectives",
    "crumbs": [
      "Class 1",
      "Class 1"
    ]
  },
  {
    "objectID": "class1/class1_overview.html#todays-objectives",
    "href": "class1/class1_overview.html#todays-objectives",
    "title": "Class 1",
    "section": "",
    "text": "Theory\n\nWhy scientific coding?\nFundamentals of scientific computing\nWhy Python?\nHow to use Python?",
    "crumbs": [
      "Class 1",
      "Class 1"
    ]
  },
  {
    "objectID": "class1/pandas1.html",
    "href": "class1/pandas1.html",
    "title": "1  Intro to Pandas",
    "section": "",
    "text": "1.1 Introduction\nWe will start our data science journey by learning a bit about the most useful Python library for this class: Pandas. As a reminder, a library is a set of tools we load on top of Python that provides new functionalities for a specific problem or type of analysis. Here, Pandas provides functions for data manipulation and analysis, handling structured data like tables or time series and facilitating numerous tasks you might encounter as a scientist. These include:",
    "crumbs": [
      "Class 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to Pandas</span>"
    ]
  },
  {
    "objectID": "class1/pandas1.html#introduction",
    "href": "class1/pandas1.html#introduction",
    "title": "1  Intro to Pandas",
    "section": "",
    "text": "Reading/writing data from various commonly-used formats (CSV, Excel, SQL, JSON, etc.)\nHandling missing data\nFiltering, sorting, reshaping and grouping data\nAggregating data (sum, mean, count, etc.)\nTime series support (date ranges, frequency conversions)\nStatistical operations\n\n\nToday’s objectives\nThe objective of this class is by no way to make you an expert in Pandas and data science. Rather, the objective is to take you through the most basic manipulations in order to build the confidence to keep on exploring the use of scientific coding and to include it into your research pipeline. The objectives of this module are to review:\n\nWhat is a Pandas DataFrame and its basic anatomy\nHow to load data in a DataFrame\nHow to access data (e.g., query by label/position)\nHow to filter data (e.g., comparison and logical operators)\nHow to rearrange data (e.g., sorting values)\nHow to operate on data (e.g., arithmetic and string operations)\n\nWe first start by reviewing the data structure behind Pandas, then we will move on to a coding exercise to make you familiar with some basic functionalities.",
    "crumbs": [
      "Class 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to Pandas</span>"
    ]
  },
  {
    "objectID": "class1/pandas1.html#pandas-data-structure",
    "href": "class1/pandas1.html#pandas-data-structure",
    "title": "1  Intro to Pandas",
    "section": "1.2 Pandas data structure",
    "text": "1.2 Pandas data structure\nPandas consists of two main types of data structures. Let’s make an analogy with Excel.\n\nSeries: A 1D labeled array. Think of a 2-columns Excel spreadsheet where the left column would contain a label (e.g., the time of a measurement) and the right column would contain a value (e.g., the actual value measured at the time specified in the label, let’s say the temperature of a river).\nDataFrame: A 2D labeled table. This is the same as an Excel spreadsheet that would contain more columns than a Series. You can think of having measurements of different variables contained in each column (e.g., the flow rate, the turbidity etc…).\n\nThe keyword here is labelled. In Excel, you might get a column using letters and rows using numbers. In Pandas, you can use the column name (e.g., water_temperature) or the row label (e.g., 2021-06-15 14:19:14).\n\n\n\n\n\n\nDataFrame\n\n\n\nThroughout this class we will focus on the use of DataFrames, not Series. Keep in mind that the behaviour between both is almost identical.\n\n\n\nAnatomy of a DataFrame\nFigure 1.1 shows the basic anatomy of a DataFrame that contains four rows and four columns). We already see some data structuring emerging:\n\nRows tend to represent entries, which can be:\n\nDifferent measurements at specific time steps\nDifferent samples collected at different place/times\netc.\n\nIn contrast, column represent attributes and store the properties of each entry:\n\nThe actual values of different measured parameters\nThe location and time of collected samples, along with associated analyses (e.g., geochemistry)\netc.\n\n\n\n\n\n\n\n\nFigure 1.1: Basic anaotmy of a Pandas DataFrame.\n\n\n\nThe first row - i.e. the row containing the column labels - is not considered as an entry. This is because the top row of a dataframe is usually used as the label for the columns. Similarly, we might want to set the first column as a label for the rows (Figure 1.2). In a nutshell:\n\nIndex refers to the label of the rows. In the index, values are usually unique - meaning that each entry has a different label.\nColumn refers to the label of - logically - the columns\n\n\n\n\n\n\n\nFigure 1.2: Index and columns of a DataFrame.\n\n\n\n\n\n\n\n\n\nCaution 1.1: Indexing in Python\n\n\n\nRemember that in Python, indexing starts from 0 - so the first row or column has an index of 0.",
    "crumbs": [
      "Class 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to Pandas</span>"
    ]
  },
  {
    "objectID": "class1/pandas1.html#coding-playground",
    "href": "class1/pandas1.html#coding-playground",
    "title": "1  Intro to Pandas",
    "section": "1.3 Coding playground",
    "text": "1.3 Coding playground\nLet’s get our hands dirty and start coding. Create a new Jupyter notebook following this guide. You can copy fragments of the code, but make sure each code block is a different cell in you notebook. Also remember that you can add Markdown cells in between code cells, which are really useful to document your code.\nThe data we will use here is a csv file containing selected eruptions of the past 50 years. The first 5 rows of the data are illustrated in Table 1.1.\n\n\n\nTable 1.1: First 5 rows of the dataset.\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nCountry\nDate\nVEI\nLatitude\nLongitude\n\n\n\n\nSt. Helens\nUSA\n1980-05-18 00:00:00\n5\n46.1914\n-122.196\n\n\nPinatubo\nPhilippines\n1991-04-02 00:00:00\n6\n15.1501\n120.347\n\n\nEl Chichón\nMexico\n1982-03-28 00:00:00\n5\n17.3559\n-93.2233\n\n\nGalunggung\nIndonesia\n1982-04-05 00:00:00\n4\n-7.2567\n108.077\n\n\nNevado del Ruiz\nColombia\n1985-11-13 00:00:00\n3\n4.895\n-75.322\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the VEI?\n\n\n\n\n\nThe Volcanic Explosivity Index - or VEI - is a scale to measure the magnitude of explosive eruptions based on the volume of tephra ejected during an eruption. It is a logarithmic scale in base 10:\n\n\n\nTable 1.2: VEI scale with minimum and maximum erupted volume and approximate frequency.\n\n\n\n\n\nVEI\nMin Volume (km³)\nMax Volume (km³)\nApprox. Frequency\n\n\n\n\n0\n&lt;0.00001\n0.0001\nDaily\n\n\n1\n0.0001\n0.001\nWeekly\n\n\n2\n0.001\n0.01\nYearly\n\n\n3\n0.01\n0.1\nFew per year\n\n\n4\n0.1\n1\n~10 per decade\n\n\n5\n1\n10\n~1 per decade\n\n\n6\n10\n100\n~1 per century\n\n\n7\n100\n1000\n~1 per several centuries\n\n\n8\n&gt;1000\n-\n~1 per 10,000 years\n\n\n\n\n\n\n\n\n\n\nImporting the library and the data\nAs always, we start by importing the pandas library as pd.\n\nimport pandas as pd\n\nWe load the dataset using the pd.read_csv function into a variable called df (for DataFrame) (doc). Remember that functions can take different arguments, which are extra keywords you can pass to make the behaviour of the function more specific to your need. Here, we pass one arguments to the read_csv() function: parse_dates=['Date'] Specifies that the Date column should be treated as a date object.\n\n\n\n\n\nListing 1.1: Loading data from a csv file\n\n\ndf = pd.read_csv('data/dummy_volcanoes.csv', parse_dates=['Date']) # Load data\ndf.head() # Show the first 5 rows\n\n\n\n\n\n\n\n\n\n\n\nName\nCountry\nDate\nVEI\nLatitude\nLongitude\n\n\n\n\n0\nSt. Helens\nUSA\n1980-05-18\n5\n46.1914\n-122.1956\n\n\n1\nPinatubo\nPhilippines\n1991-04-02\n6\n15.1501\n120.3465\n\n\n2\nEl Chichón\nMexico\n1982-03-28\n5\n17.3559\n-93.2233\n\n\n3\nGalunggung\nIndonesia\n1982-04-05\n4\n-7.2567\n108.0771\n\n\n4\nNevado del Ruiz\nColombia\n1985-11-13\n3\n4.8950\n-75.3220\n\n\n\n\n\n\n\n\nSetting up the index\nThe output of Listing 1.1 shows the first 5 rows in our DataFrame. As displayed in Figure 1.2, the first column is the index - which is currently just integer numbers. That can be acceptable in some cases, but for the sake of the exercise we will choose one column to become the index - here Name.\nListing 1.2 Illustrates the use of two useful functions:\n\n.set_index(): Uses a column as the DataFrame’s index\n.reset_index(): Removes the column’s index back to a sequential numbering as in Listing 1.1.\n\n\n\n\n\nListing 1.2: Common functions to set the index of a DataFrame\n\n\ndf = df.set_index('VEI') # Set the 'VEI' column as an index\ndf = df.reset_index() # Shoots, I meant to set the 'Name' columns as an index\ndf = df.set_index('Name') # Here we go.\ndf.head()\n\n\n\n\n\n\n\n\n\n\n\nVEI\nCountry\nDate\nLatitude\nLongitude\n\n\nName\n\n\n\n\n\n\n\n\n\nSt. Helens\n5\nUSA\n1980-05-18\n46.1914\n-122.1956\n\n\nPinatubo\n6\nPhilippines\n1991-04-02\n15.1501\n120.3465\n\n\nEl Chichón\n5\nMexico\n1982-03-28\n17.3559\n-93.2233\n\n\nGalunggung\n4\nIndonesia\n1982-04-05\n-7.2567\n108.0771\n\n\nNevado del Ruiz\n3\nColombia\n1985-11-13\n4.8950\n-75.3220\n\n\n\n\n\n\n\n\n\nBasic data exploration\nLet’s now explore the structure of the dataset with the following functions:\n\n\n\nFunction\nDescription\n\n\n\n\ndf.head()\nPrints the first 5 rows of the DataFrame (doc)\n\n\ndf.tail()\nPrints the last 5 rows of the DataFrame (doc)\n\n\ndf.info()\nDisplays some info about the DataFrame, including the number of rows (entries) and columns (doc). Note the Dtype column: this is the type variable stored in each column including strings (object), integer (int64) and float (int64) numbers. See that the Date column is indeed stored as a datetime variable as requested above.\n\n\ndf.shape\nReturns a list containing the number of rows and columns of the DataFrame.\n\n\ndf.index\nReturns a list containing the index along the rows of the DataFrame.\n\n\ndf.columns\nReturns a list containing the index along the columns of the DataFrame.\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nTry these functions on df and get familiar with the output.\n\n\n\n\n\nQuerying data\nLet’s now review how we can access data contained in the DataFrame. This process, known as indexing, consists in specifying a row or a column (or ranges of rows and columns) where the data is stored. In pandas, there are two different ways to do that:\n\nBy label: data is queried using the actual index/column name (e.g., the VEI column in the DataFrame above)\nBy location: data is queried using the column location (e.g., the 3rd row)\n\n\nLabel-based indexing\n\n\n\n\n\n\nFigure 1.3: Label-based queries using .loc.\n\n\n\n\nQuerying rows\nWhen we know the exact label of the row or the column, we can use the .loc function to query the DataFrame (Figure 1.3). Let’s start by querying specific rows. Listing 1.2 has defined the Name column as the index (i.e., row label), which means that we can simply pass the name of the volcano.\n\n# Get the row for \"Calbuco\" volcano\ndf.loc['Calbuco']\n\nVEI                            4\nCountry                    Chile\nDate         2015-04-22 00:00:00\nLatitude                -41.2972\nLongitude               -72.6097\nName: Calbuco, dtype: object\n\n\nNote that the result is a Series (i.e., a 1-dimensional DataFrame where the columns become the index), not a DataFrame. If we want to keep it as a DataFrame, we can use double brackets. Double brackets can also be used to query multiple rows.\n\ndf.loc[['Calbuco']] # Query one row and return it as a DataFrame\ndf.loc[['Calbuco', 'Taal']] # Query multiple rows\n\n\n\n\n\n\n\n\nVEI\nCountry\nDate\nLatitude\nLongitude\n\n\nName\n\n\n\n\n\n\n\n\n\nCalbuco\n4\nChile\n2015-04-22\n-41.2972\n-72.6097\n\n\nTaal\n4\nPhilippines\n2020-01-12\n14.0020\n120.9934\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the VEI recorded for Etna volcano?\n\n\n12345\n\n\nWhat is the eruption date for Taal volcano?\n\n\n1980-05-18    1991-04-022020-01-121997-06-252023-02-13\n\n\n\n\n\n\n\n\n\n\nDouble brackets\n\n\n\nIn general, double brackets [[ ]] will return a DataFrame and not a Series. We will dominantly use this in the following examples.\n\n\n\n\nQuerying columns\nLet’s now query specific columns. For example, querying the VEI column can be achieved in different ways. The simplest is to use the column name directly. We can also query multiple columns using double brackets\n\ndf[['VEI']] # Get the VEI column\ndf[['Country', 'VEI']] # Query multiple columns\n\n\n\n\n\n\n\n\nCountry\nVEI\n\n\nName\n\n\n\n\n\n\nSt. Helens\nUSA\n5\n\n\nPinatubo\nPhilippines\n6\n\n\nEl Chichón\nMexico\n5\n\n\nGalunggung\nIndonesia\n4\n\n\nNevado del Ruiz\nColombia\n3\n\n\nMerapi\nIndonesia\n2\n\n\nOntake\nJapan\n2\n\n\nSoufrière Hills\nMontserrat\n3\n\n\nEtna\nItaly\n2\n\n\nNyiragongo\nDR Congo\n1\n\n\nKīlauea\nUSA\n2\n\n\nAgung\nIndonesia\n3\n\n\nTavurvur\nPapua New Guinea\n3\n\n\nSinabung\nIndonesia\n3\n\n\nTaal\nPhilippines\n4\n\n\nLa Soufrière\nSaint Vincent\n4\n\n\nCalbuco\nChile\n4\n\n\nSt. Augustine\nUSA\n3\n\n\nEyjafjallajökull\nIceland\n4\n\n\nCleveland\nUSA\n3\n\n\n\n\n\n\n\nNote that until now, we have only retrieved either rows or columns (Figure 1.3). We can also retrieve specific values by specifying both the row and the column.\n\ndf.loc[['Calbuco', 'Taal']][['Country', 'VEI']]\n\n\n\n\n\n\n\n\nCountry\nVEI\n\n\nName\n\n\n\n\n\n\nCalbuco\nChile\n4\n\n\nTaal\nPhilippines\n4\n\n\n\n\n\n\n\n\n\n\nPosition-based indexing\n\n\n\n\n\n\nFigure 1.4: Position-based queries using .iloc.\n\n\n\nSome situations require querying data by location instead of label - let’s say for instance we need to retrieve rows 10-20. This is done using the .iloc function (instead of the .loc function previously used; Figure 1.4). Remember that Python uses zero-based indexing (Caution 1.1), meaning that the first element is at position 0, the second at position 1, and so on.\nThe next example queries the first row - using again double brackets to return a DataFrame.\n\ndf.iloc[[0]]\n\n\n\n\n\n\n\n\nVEI\nCountry\nDate\nLatitude\nLongitude\n\n\nName\n\n\n\n\n\n\n\n\n\nSt. Helens\n5\nUSA\n1980-05-18\n46.1914\n-122.1956\n\n\n\n\n\n\n\n\nGet ranges of rows\nWe can get a range of rows using what is called slicing. This is done using the colon (:) operator. The next example queries rows 3 to 6 of the DataFrame. Note that the end index is exclusive, meaning that the element at the end index is not included in the result.\n\ndf.iloc[2:6]\n\n\n\n\n\n\n\n\nVEI\nCountry\nDate\nLatitude\nLongitude\n\n\nName\n\n\n\n\n\n\n\n\n\nEl Chichón\n5\nMexico\n1982-03-28\n17.3559\n-93.2233\n\n\nGalunggung\n4\nIndonesia\n1982-04-05\n-7.2567\n108.0771\n\n\nNevado del Ruiz\n3\nColombia\n1985-11-13\n4.8950\n-75.3220\n\n\nMerapi\n2\nIndonesia\n2023-12-03\n-7.5407\n110.4457\n\n\n\n\n\n\n\nTo get rows 3 to 6 and columns 2-3:\n\ndf.iloc[0:5, 1:3]\n\n\n\n\n\n\n\n\nCountry\nDate\n\n\nName\n\n\n\n\n\n\nSt. Helens\nUSA\n1980-05-18\n\n\nPinatubo\nPhilippines\n1991-04-02\n\n\nEl Chichón\nMexico\n1982-03-28\n\n\nGalunggung\nIndonesia\n1982-04-05\n\n\nNevado del Ruiz\nColombia\n1985-11-13\n\n\n\n\n\n\n\n\n\nCount rows from the last\nTo get the last 5 rows of the DataFrame:\n\ndf.iloc[-5:]\n\n\n\n\n\n\n\n\nVEI\nCountry\nDate\nLatitude\nLongitude\n\n\nName\n\n\n\n\n\n\n\n\n\nLa Soufrière\n4\nSaint Vincent\n2021-04-09\n13.2833\n-61.3875\n\n\nCalbuco\n4\nChile\n2015-04-22\n-41.2972\n-72.6097\n\n\nSt. Augustine\n3\nUSA\n2006-03-27\n57.8819\n-155.5611\n\n\nEyjafjallajökull\n4\nIceland\n2010-04-14\n63.6333\n-19.6111\n\n\nCleveland\n3\nUSA\n2023-05-23\n52.8250\n-169.9444\n\n\n\n\n\n\n\n\n\nCombining position-based and label-based queries\nBy experience, position-based queries is more used on rows than columns. For instance, we might want to access the first 10 rows because we don’t know their associated labels, yet it is less likely that we ignore their attributes. It is possible mix label-based and position-based indexing. For example, to get the first 5 rows and the Country and VEI columns:\n\ndf.iloc[0:5][['Country', 'VEI']]\n\n\n\n\n\n\n\n\nCountry\nVEI\n\n\nName\n\n\n\n\n\n\nSt. Helens\nUSA\n5\n\n\nPinatubo\nPhilippines\n6\n\n\nEl Chichón\nMexico\n5\n\n\nGalunggung\nIndonesia\n4\n\n\nNevado del Ruiz\nColombia\n3\n\n\n\n\n\n\n\n\n\n\n\nFiltering data\n\nComparison operators\nNow that we have reviewed how to access data, let’s now see how to filter data using boolean indexing. For this, we need to review what are comparison operators (Table 1.3). Let’s assume the following variables:\n\n\n\n\nListing 1.3: Variables used for illustrating logical operations\n\n\na = 1\nb = 2\n\n\n\n\nApplying the comparison operators in Table 1.3 will produce a variable of type bool - which can take only two values: True or False.\n\n\n\nTable 1.3: Comparison operators in Python.\n\n\n\n\n\nOperator\nMeaning\nExample\nResult\n\n\n\n\n==\nEqual to\na == b\nFalse\n\n\n!=\nNot equal to\na != b\nTrue\n\n\n&gt;\nGreater than\na &gt; b\nFalse\n\n\n&lt;\nLess than\na &lt; b\nTrue\n\n\n&gt;=\nGreater than or equal\na &gt;= b\nFalse\n\n\n&lt;=\nLess than or equal\na &lt;= b\nTrue\n\n\n\n\n\n\nWe can apply comparison operators to DataFrame. Let’s say we want to test what rows have a VEI of 4:\n\n\n\n\nListing 1.4: Create a boolean mask\n\n\ndf['VEI'] == 4\n\n\n\n\nName\nSt. Helens          False\nPinatubo            False\nEl Chichón          False\nGalunggung           True\nNevado del Ruiz     False\nMerapi              False\nOntake              False\nSoufrière Hills     False\nEtna                False\nNyiragongo          False\nKīlauea             False\nAgung               False\nTavurvur            False\nSinabung            False\nTaal                 True\nLa Soufrière         True\nCalbuco              True\nSt. Augustine       False\nEyjafjallajökull     True\nCleveland           False\nName: VEI, dtype: bool\n\n\nWe can see that Galunggung, Taal, La Soufrière, Calbuco and Eyjafjallajökull return True to this condition. This is great, but what if we want to return the actual rows? We can use Listing 1.4 as a mask to then query the rows using .loc.\n\n\n\n\nListing 1.5: Query data using a boolean mask\n\n\nmask = df['VEI'] == 4 # Create a mask\ndf.loc[mask] # Query the data\n\n# Or, as a one line alternative:\ndf.loc[df['VEI'] == 4]\n\n\n\n\n\n\n\n\n\n\n\nVEI\nCountry\nDate\nLatitude\nLongitude\n\n\nName\n\n\n\n\n\n\n\n\n\nGalunggung\n4\nIndonesia\n1982-04-05\n-7.2567\n108.0771\n\n\nTaal\n4\nPhilippines\n2020-01-12\n14.0020\n120.9934\n\n\nLa Soufrière\n4\nSaint Vincent\n2021-04-09\n13.2833\n-61.3875\n\n\nCalbuco\n4\nChile\n2015-04-22\n-41.2972\n-72.6097\n\n\nEyjafjallajökull\n4\nIceland\n2010-04-14\n63.6333\n-19.6111\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat volcanoes have a VEI of 5?\n\n\nLa Soufrière and CalbucoMerapi and AgungNyiragongo and TaalSt. Helens and El Chichón\n\n\nHow many volcanoes are in the southern hemisphere? (hint: use the df.shape function to count them).\n\n\n124612\n\n\n\n\n\nString comparisons\nWe can also use comparison operators on columns containing strings (see Caution 1.2 for caveats). Listing 1.6 illustrates a basic string comparison using the = operator. Table 1.4 shows additional operators for strings.\n\n\n\n\nListing 1.6: Basic comparison operation on strings\n\n\ndf.loc[df['Country'] == 'Indonesia']\n\n\n\n\n\n\n\n\n\n\n\nVEI\nCountry\nDate\nLatitude\nLongitude\n\n\nName\n\n\n\n\n\n\n\n\n\nGalunggung\n4\nIndonesia\n1982-04-05\n-7.2567\n108.0771\n\n\nMerapi\n2\nIndonesia\n2023-12-03\n-7.5407\n110.4457\n\n\nAgung\n3\nIndonesia\n2017-11-21\n-8.3422\n115.5083\n\n\nSinabung\n3\nIndonesia\n2023-02-13\n3.1719\n98.3925\n\n\n\n\n\n\n\n\n\n\nTable 1.4: Common string comparison operations.\n\n\n\n\n\n\n\n\n\n\nOperation\nExample\nDescription\n\n\n\n\nContains\ndf['Name'].str.contains('Soufrière')\nChecks if each string contains a substring\n\n\nStartswith\ndf['Name'].str.startswith('E')\nChecks if each string starts with a substring\n\n\nEndswith\ndf['Name'].str.endswith('o')\nChecks if each string ends with a substring\n\n\n\n\n\n\n\n\n\n\n\n\nCaution 1.2: Compare what is comparable!\n\n\n\n\n\nWhen using the comparison operators in Table 1.3, we need to make sure that we are comparing data that have the same type. In Listing 1.4, we are comparing the column VEI with an integer number. You can check the data type of a DataFrame using df.dtypes.\nNot all comparison operators work with all data type. For instance, you can test if a column contains a specific string using the == or != operators, but the other won’t work as they are illogical.\n\n\n\n\n\n\nLogical operators\nBut what if we want to create more complex filters based on different rules? We can use logical operators to combine several comparison operators. Going back to the example in Listing 1.3, Table 1.5 illustrates the use of logical operators.\n\n\n\nTable 1.5: Logical operators in pandas for combining boolean conditions. Use parentheses around each condition.\n\n\n\n\n\n\n\n\n\n\n\nOperator\nMeaning\nExample\nResult\n\n\n\n\n&\nLogical AND\n(a &gt; 1) & (b &lt; 3)\nFalse\n\n\n|\nLogical OR\n(a == 1) | (b == 1)\nTrue\n\n\n~\nLogical NOT\n~(a == 1)\nFalse\n\n\n\n\n\n\nLet’s gather all volcanoes that have a VEI of 3 and are in Indonesia:\n\n\n\n\nListing 1.7: Complex filtering using logical operators\n\n\nmask = (df['VEI'] == 3) & (df['Country'] == 'Indonesia') # Create a mask - don't forget parentheses!\ndf.loc[mask] # Query the data\n\n\n\n\n\n\n\n\n\n\n\nVEI\nCountry\nDate\nLatitude\nLongitude\n\n\nName\n\n\n\n\n\n\n\n\n\nAgung\n3\nIndonesia\n2017-11-21\n-8.3422\n115.5083\n\n\nSinabung\n3\nIndonesia\n2023-02-13\n3.1719\n98.3925\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow many volcanoes are either in Chile or in the USA?\n\n\n1358\n\n\nHow many volcanoes are in the southern hemisphere and have a VEI≥4?\n\n\n12812\n\n\n\n\n\n\n\nRearranging data\n\nSorting data\nThe main function to sort data is .sort_values (doc). It is necessary to review how three arguments can alter the function’s behaviour:\n\nby: First argument (required) is the label of index/row used to sort the data. It is possible to sort by multiple columns by passing a list of values.\naxis: Specifies whether sorting rows (axis = 0 - in which case by is a column name) or sorting columns (axis = 1, in which case by is an index value). The documentation specifies axis = 0, which means that rows will be sorted if axis is not specified.\nascending: Using a bool (remember, this is a True/False behaviour), specifies if values are sorted in ascending (ascending = True, default behaviour is not specified) or descending (ascending = False) order.\n\n\n\n\n\nListing 1.8: Basic sorting operations\n\n\ndf.sort_values('VEI') # Sort volcanoes by VEI in ascending number\ndf.sort_values('Date', ascending=False) # Sort volcanoes by eruption dates from recent to old\ndf.sort_values('Country') # .sort_values also work on strings to sort alphabetically\ndf.sort_values(['Latitude', 'Longitude']) # Sorting using multiple columns\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nAfter sorting the data in descending order by VEI and time, what are the three first volcanoes?\n\n\nNyiragongo, Ontake, KīlaueaKīlauea, Ontake, NyiragongoPinatubo, El Chichon, St HelensSt Helens, El Chichon, Pinatubo\n\n\n\n\n\n\n\nOperations\nLet’s now see how we can manipulate and operate on data contained within our DataFrame. Table 1.6 and Table 1.7 respectively illustrate arithmetic and string-based operators that can be applied on parts of the DataFrame.\n\nNumeric operations\nListing 1.9 Illustrates how to half the VEI column save the results to a new column.\n\n\n\n\nListing 1.9: Divide VEI by two and save the results to a new column.\n\n\ndf['VEI_halved'] = df['VEI'] / 2\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nLongitudes are expressed as degrees E (i.e., from 0–180 ) and degrees W (i.e., from -180–0). Use operators to convert longitudes to degrees E (i.e., from 0–360) and store the results to a column called Longitude_E. To do so:\n\nDefine a mask where Longitudes are negative using logical operators\nWhere the mask is True (i.e. where the longitude is negative), add the longitude (or subtract its absolute value) to 360\n\n\n\n\n\n\n\nDefine a mask\n\n\n\n\n\nStart by defining a mask\n\n\n\n\n\n\nHow?\n\n\n\n\n\nmask = df['Longitude'] &lt;= 0\n\n\n\n\n\n\n\n\n\n\n\n\nSelect the values\n\n\n\n\n\nSelect the values using .loc and do the maths.\n\n\n\n\n\n\nHow?\n\n\n\n\n\n360 + df.loc[mask, 'Longitude']\n\n\n\n\n\n\n\n\n\n\n\n\nStore back the values\n\n\n\n\n\ndf.loc[mask, 'Longitude_E'] = 360 + df.loc[mask, 'Longitude']\n\n\n\n\n\n\n\n\nTable 1.6: Common arithmetic operations on numerical pandas columns.\n\n\n\n\n\n\n\n\n\n\n\nOperation\nSymbol\nExample\nDescription\n\n\n\n\nAddition\n+\ndf['VEI'] + 1\nAdds a value to each element\n\n\nSubtraction\n-\ndf['VEI'] - 1\nSubtracts a value from each element\n\n\nMultiplication\n*\ndf['VEI'] * 2\nMultiplies each element by a value\n\n\nDivision\n/\ndf['VEI'] / 2\nDivides each element by a value\n\n\nExponentiation\n**\ndf['VEI'] ** 2\nRaises each element to a power\n\n\nModulo\n%\ndf['VEI'] % 2\nRemainder after division for each element\n\n\n\n\n\n\n\n\nString operations\n\n\n\nTable 1.7: Common string operations on pandas columns.\n\n\n\n\n\n\n\n\n\n\nOperation\nExample\nDescription\n\n\n\n\nConcatenation\ndf['Country'] + ' volcano'\nAdds a string to each element\n\n\nString length\ndf['Country'].str.len()\nReturns the length of each string\n\n\nUppercase\ndf['Country'].str.upper()\nConverts each string to uppercase\n\n\nLowercase\ndf['Country'].str.lower()\nConverts each string to lowercase\n\n\nReplace\ndf['Country'].str.replace('USA', 'US')\nReplaces substrings in each string",
    "crumbs": [
      "Class 1",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Intro to Pandas</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.",
    "crumbs": [
      "Class 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "appendices/setup_vscode.html",
    "href": "appendices/setup_vscode.html",
    "title": "Setup VS Code",
    "section": "",
    "text": "Open VS Code\nThis tutorial guides you through the installation and configuration of Visual Studio Code (VS Code) for Python development in the 0013 computer room at the University of Geneva.\nThese steps will ensure that Python works and finds the files in this exact folder.",
    "crumbs": [
      "Appendices",
      "Setup VS Code"
    ]
  },
  {
    "objectID": "appendices/setup_vscode.html#open-vs-code",
    "href": "appendices/setup_vscode.html#open-vs-code",
    "title": "Setup VS Code",
    "section": "",
    "text": "Create a folder on your H:/ drive named data_science\nOpen VS Code\nClick File / Open Folder and choose the data_science folder (Figure 1)\nWhen asked, click to confirm your trust in the system\n\n\n\n\n\n\n\n\nFigure 1: Open VS Code and set the working folder",
    "crumbs": [
      "Appendices",
      "Setup VS Code"
    ]
  },
  {
    "objectID": "appendices/setup_vscode.html#create-a-new-jupyter-notebook",
    "href": "appendices/setup_vscode.html#create-a-new-jupyter-notebook",
    "title": "Setup VS Code",
    "section": "Create a new Jupyter Notebook",
    "text": "Create a new Jupyter Notebook\n\nType Ctrl + Shift + i (or Cmd + Shift + i on MacOS) to open the command palette\nType jupyter and select Create: New Jupyter Notebook (Figure 2)\n\n\n\n\n\n\n\nFigure 2: Create a new Jupyter Notebook\n\n\n\n\n\n\n\n\n\nCommand palette\n\n\n\nThe command palette is a key functionality of VS Code, learning to use it will speed up your workflow.",
    "crumbs": [
      "Appendices",
      "Setup VS Code"
    ]
  },
  {
    "objectID": "appendices/setup_vscode.html#install-the-python-extensions",
    "href": "appendices/setup_vscode.html#install-the-python-extensions",
    "title": "Setup VS Code",
    "section": "Install the Python extensions",
    "text": "Install the Python extensions\nThis step installs the Python extensions for VS Code.\n\nWhen asked, click the Install button and wait for it to finish (Figure 3)\n\n\n\n\n\n\n\n\nFigure 3: Create a new Jupyter Notebook",
    "crumbs": [
      "Appendices",
      "Setup VS Code"
    ]
  },
  {
    "objectID": "appendices/setup_vscode.html#install-jupyter",
    "href": "appendices/setup_vscode.html#install-jupyter",
    "title": "Setup VS Code",
    "section": "Install Jupyter",
    "text": "Install Jupyter\nJupyter Notebooks are one of the ways to run Python. This step installs all the required Jupyter stuff for VS Code.\n\nOn the top right corner of the notebook, locate and click on the Select kernel button Figure 4\nInstall the Jupyter notebook support extension (Figure 5)\n\n\n\n\n\n\n\nFigure 4: Browse for kernels\n\n\n\n\n\n\n\n\n\nFigure 5: Install the Jupyter notebook support extension",
    "crumbs": [
      "Appendices",
      "Setup VS Code"
    ]
  },
  {
    "objectID": "appendices/setup_vscode.html#select-the-right-python",
    "href": "appendices/setup_vscode.html#select-the-right-python",
    "title": "Setup VS Code",
    "section": "Select the right Python",
    "text": "Select the right Python\nSeveral versions of Python can be installed on a computer. In the computer room, these versions are controlled by an environment manager called anaconda. This step ensures that the Jupyter notebook will use the right Python version, i.e., the one installed using anaconda.\n\nOn the top right corner of the notebook, where the Select kernel button previously appeared, make sure you see base (Python 3.x.x) (Figure 6)\nIf you don’t, click on the button and search for the right one (i.e., there should be anaconda in the path)\n\n\n\n\n\n\n\nFigure 6: Select the right Python environment",
    "crumbs": [
      "Appendices",
      "Setup VS Code"
    ]
  },
  {
    "objectID": "class1/pandas2.html",
    "href": "class1/pandas2.html",
    "title": "2  Data exploration",
    "section": "",
    "text": "2.1 Introduction",
    "crumbs": [
      "Class 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "class1/pandas2.html#introduction",
    "href": "class1/pandas2.html#introduction",
    "title": "2  Data exploration",
    "section": "",
    "text": "Today’s objectives",
    "crumbs": [
      "Class 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "class1/pandas2.html#setting-up-the-exercise",
    "href": "class1/pandas2.html#setting-up-the-exercise",
    "title": "2  Data exploration",
    "section": "2.2 Setting up the exercise",
    "text": "2.2 Setting up the exercise\nWe start by importing libraries and the dataset.\n\n\n\n\nListing 2.1: caption\n\n\n# Load libraries\nimport pandas as pd # Import pandas\nimport matplotlib.pyplot as plt # Import the pyplot module from matplotlib\nimport seaborn as sns # Import seaborn\n\n# Import the dataset specifying which Excel sheet name to load the data from\ndf = pd.read_excel('../Data/Smith_glass_post_NYT_data.xlsx', sheet_name='Supp_traces')\n\n\n\n\nAs in the previous class, we load the dataset using Pandas. The dataset comes from Smith, Isaia, and Pearce (2011) and contains the chemical concentrations in volcanic tephra belonging to the recent activity (last 15 ky) of the Campi Flergrei Caldera (Italy). The dataset is contained in an Excel file that contains two sheets named 'Supp_majors' and 'Supp_traces'. In Listing 2.1, we use the sheet_name argument to the read_excel() (doc) function to specify that we want to load the sheet containing trace elements.\n\n\n\n\n\n\nGood reference book\n\n\n\nThe use of this dataset is inspired by the by the book Introduction to Python in Earth Science Data Analysis: From Descriptive Statistics to Machine Learning by Petrelli (2021).\n\n\n\nBasics of plotting in Python\nListing 2.1 also loads two libraries for plotting:\n\nMatplotlib is the library for plotting in Python. It allows for the finest and most comprehensive level of customisation, but it take some time to get used to. You can visit the gallery to get some inspiration.\nSeaborn is built on Matplotlib, but provides some higher-level functions to easily produce stats-oriented plots. It looks good by default, but finer customisation might be difficult and might result to using Matplotlib. Again, check out the gallery.\n\nHere again, the idea is not to make you expert in Matplotlib or Seaborn, but rather to provide you with the minimum knowledge for you to further explore this tools in the context of your research.\n\nComponents of a Figure\nLet’s start to look at the basic components of a Matplotlib figure. There are two “hosts” to any plot (Figure 2.1):\n\nA Figure represents the main canevas;\nAxes are contained within the figure and is where most of the plotting will occur.\n\nThe easiest way to define a figure is using the subplots() function (Listing 2.2). Note that the code returns two variables - fig and ax - which are the figure and the axes, respectively.\n\n\n\n\nListing 2.2: Define a figure and one axes.\n\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots()\n\n\n\n\n\n\n\n\n\n\nFigure 2.1: Main figure and axes of a Matplotlib figure\n\n\n\nMost additional components of a plot are controlled via the ax variable, which can be used to (Figure 2.2):\n\nPlot (e.g., line plot or scatter plots)\nSet labels (e.g., x-label, y-label, title)\nAdd various components (e.g., legend, grid)\n\n\n\n\n\n\n\nFigure 2.2: Basic components of a Matplotlib figure\n\n\n\n\n\n\n\n\n\nPlotting exercise\n\n\n\nListing 2.3 defines a figure and plots some data. Table 2.1 and Figure 2.2 illustrate some of the most frequently used functions for customising plots.\nUse these functions to customise Listing 2.3. Some hints:\n\nRemember that a function takes some arguments provided between parentheses (e.g., ax.title(argument)). Each function might accept different types of arguments.\nTitles and labels require a string, so remember to use \" \" or ' '.\nFor now, the legend does not require any argument, so you can leave the parentheses empty.\nSetting the grid requires one argument: do we want to show the grid (True) or not (False)\n\n\n\n\n\n\nTable 2.1: Some of the most frequently used plotting functions.\n\n\n\n\n\nFunction\nDescription\nArgument Type\nExample Argument\n\n\n\n\nax.set_title\nSets the title of the axes\nstr\n“My Plot Title”\n\n\nax.set_xlabel\nSets the label for the x-axis\nstr\n“X Axis Label”\n\n\nax.set_ylabel\nSets the label for the y-axis\nstr\n“Y Axis Label”\n\n\nax.legend\nDisplays the legend\nNone\nNone (default)\n\n\nax.grid\nShows grid lines\nbool\nTrue\n\n\n\n\n\n\n\n\n\n\nListing 2.3: Define a figure and one axes.\n\n\nimport matplotlib.pyplot as plt # Import matplotlib\n\n# Define some data\ndata1 = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\ndata2 = [7, 3, 9, 1, 5, 10, 8, 2, 6, 4]\n\n# Set the figure and the axes\nfig, ax = plt.subplots()\n\n# Plot the data\nax.plot(data1, data1, color='aqua', label='Line')\nax.scatter(data1, data2, color='purple', label='scatter')\n\n# Customise the plot - up to you!\n# - Add a title\n# - Add x and y labels\n# - Add a legend\n# - Add a grid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting with Seaborn\nLet’s now review the use of Seaborn. You might wonder why do we need another plotting library. Well, the topic of this module is data exploration, and this is exactly what Seaborn is designed for. In additions, Seaborn is designed to work with Pandas.\nOver the next steps of the exercise we will use various types of plots offered by Seaborn to explore our geochemical dataset. Listing 2.4 illustrates how to create a scatterplot of the Rubidium and Strontium values contained in our dataset df. Seaborn usually takes 4 arguments:\n\nax: The axes on which to plot the data\ndata: The DataFrame containing the data.\nx: The name of the column containing the values used along x\ny: The name of the column containing the values used along y\n\n\n\n\n\nListing 2.4: Basic plotting using Seaborn.\n\n\n# Define figure + axes\nfig, ax = plt.subplots()\n# Plot with seaborn (remember, we imported it as sns)\n# df is the geochemical dataset imported previously\nsns.scatterplot(ax=ax, data=df, x='Rb', y='Sr')\n\n\n\n\n\n\n\n\n\n\n\nShould you feel adventurous and check out the documentation of the scatterplot function, you would see that it is possible to use additional arguments to further customize the plot:\n\nhue: Name of the variable that will produce points with different colors.\nsize: Name of the variable that will produce points with different sizes.\n\n\n\n\n\n\n\nSeaborn exercise\n\n\n\nComplete Listing 2.4, but use:\n\nThe \"Epoch\" column to control points color.\nThe \"SiO2* (EMP)\" column to control points size.\n\nRemember, you can use df.columns to print a list of column names contained in the DataFrame.",
    "crumbs": [
      "Class 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "class1/pandas2.html#basics-of-plotting-in-python",
    "href": "class1/pandas2.html#basics-of-plotting-in-python",
    "title": "2  Data exploration",
    "section": "2.3 Basics of plotting in Python",
    "text": "2.3 Basics of plotting in Python",
    "crumbs": [
      "Class 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "class1/pandas2.html#descriptive-statistics",
    "href": "class1/pandas2.html#descriptive-statistics",
    "title": "2  Data exploration",
    "section": "2.3 Descriptive statistics",
    "text": "2.3 Descriptive statistics\n\nUnivariate statistics\nThe objective of descriptive statistics is to provide ways of capturing the properties of a given data set or sample. Using Pandas and Seaborn, we will review some metrics, tools, and strategies that can be used to summarize a dataset, providing us with a quantitative basis to talk about it and compare it to other datasets.\nBy univariate statistics, we focus on capturing the properties of single variables at the time. We are not yet concerned in characterising the relationships between two or more variables.\nLet’s focus on the Zircon concentration in Campi Flegrei’s geochemical dataset. Let’s start by simply visualising the dataset to get an understanding of what we will talk about. Figures below illustrate three different plot types:\n\nHistograms (sns.histplots()) show the number of times a specific Zr concentration occurs in our dataset (Listing 2.5).\nBox plots (sns.boxplot() - or box-and-whisker plot) show the distribution of quantitative data with some measure of dispersion. Note that they are useful to compare between variables (Listing 2.6).\nViolin plots (sns.violinplot()) are similar to box plots, but they approximate the underlying data distribution using some smoothing algorithm (i.e. kernel density estimation). Without going into too much detail, this provides a good first approximation of the distribution, but it can create some unrealistic artefacts (e.g., see the negative Sr concentrations in Listing 2.7).\n\n\nHistogramBox plotViolin plot\n\n\n\n\n\n\nListing 2.5: Visualising distributions using histograms.\n\n\nfig, ax = plt.subplots()\nsns.histplot(data=df, x='Zr')\nax.set_xlabel('Zr (ppm)');\nax.set_title('Zr distribution');\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting several columns\n\n\n\nBox plots are useful to plot and compare more than one data. Listing 2.6 slightly adjusts how boxplot() is called to allow for that.\n\n\n\n\n\n\nListing 2.6: Visualising distributions using box-and-whisker plots.\n\n\nfig, ax = plt.subplots()\nsns.boxplot(data=df[['Zr', 'Sr']]) # We plot Zr and Sr together using a list\nax.set_ylabel('Concentration (ppm)');\nax.set_title('Zr and Sr distribution');\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nListing 2.7: Visualising distributions using violin plots.\n\n\nfig, ax = plt.subplots()\nsns.violinplot(data=df[['Zr', 'Sr']])\nax.set_xlabel('Concentration (ppm)');\nax.set_title('Zr and Sr distribution');\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBy looking at the Zr and St distributions from the figures above, we can intuitively understand the importance of describing three different parameters:\n\nThe location - or where is the central value(s) of the dataset;\nThe dispersion - or how spread out is the distribution of data compared to the central values;\nThe skewness - or how symmetrical is the distribution of data compared to the central values;\n\n\nLocation\n\nMean\nThe mean (or arithmetic mean - by opposition to geometric or harmonic means) is the sum of the values divided by the number of values (Equation 2.1):\n\\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\tag{2.1}\\]\nThe mean is meaningful to describe symmetric distributions without outliers, where:\n\nSymmetric means the number of items above the mean should be roughly the same as the number below;\nOutliers are extreme values.\n\nThe mean of a dataset can easily be computed with Pandas using the .mean() function (Listing 2.8). Note that we round the results to two significant digits using .round(2).\n\n\n\n\nListing 2.8: Compute the mean for two significant digits.\n\n\ndf['Zr'].mean().round(2)\n\n\n\n\n365.38\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the mean value for Strontium?\n\n\n365.38516.42219.38123.94\n\n\n\n\n\n\n\n\n\n\nDescriptive stats functions\n\n\n\nListing 2.8 shows how to compute the mean on one column, but the .mean() function - as well as most fuctions for descriptive stats - can be applied to entire DataFrames. For this, we need to understand a critical argument - axis.\n\naxis = 0 is usually the default, and computes the mean across all rows for each column(Listing 2.9)\naxis = 1 usually makes sense when rows are labelled, and computes the mean across all columns for each row(Listing 2.10)\n\n\n\n\n\nListing 2.9: Compute the mean across rows.\n\n\n# Create a subset of the df containing numerical data\ndf_sub = df[['Sc','Rb','Sr','Y','Zr','Nb','Cs']] \n# Compute the mean across all rows\ndf_sub.mean(axis=0).round(2).head()\n\n\n\n\nSc      0.20\nRb    343.82\nSr    516.42\nY      31.33\nZr    365.38\ndtype: float64\n\n\n\n\n\n\nListing 2.10: Compute the mean across columns.\n\n\n# Create a subset of the df containing numerical data\ndf_sub = df[['Sc','Rb','Sr','Y','Zr','Nb','Cs']] \n# Compute the mean across all columns\ndf_sub.mean(axis=1).round(2).head()\n\n\n\n\n0    198.32\n1    188.41\n2    188.48\n3    200.69\n4    198.42\ndtype: float64\n\n\n\n\n\n\nMedian\nThe median is the value at the exact middle of the dataset, meaning that just as many elements lie above the median as below it. There is no easy formula to compute the median, instead we need to conceptually (Listing 2.11):\n\nOrder all values in ascending order and plot them against a normalised number of observations (this is called an empirical cumulative density function - or ECDF);\nOn the x-axis, find the point dividing the dataset into two equal numbers of observations;\nRead the value on the y-axis that intersects the ECDF.\n\n\n\n\n\nListing 2.11: Graphical representation of the median.\n\n\nfig, ax = plt.subplots()\nsns.ecdfplot(data=df, y='Zr')\nax.plot([.5,.5], [0, df['Zr'].median()], color='orange', linestyle='--')\nax.plot([.5,0], [df['Zr'].median(), df['Zr'].median()], color='orange', linestyle='--')\nax.set_ylim([0,900])\n\n\n\n\n\n\n\n\n\n\n\nFortunately, we can also use the native Pandas .median function (Listing 2.12).\n\n\n\n\nListing 2.12: Compute the median for two significant digits.\n\n\ndf['Zr'].median().round(2)\n\n\n\n\n339.41\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the mean value for Cesium?\n\n\n36.3851.4223.9913.94\n\n\n\n\n\n\n\nSummary\nListing 2.13 illustrate the values of the mean and the median relative to the distribution shown in the histogram. There are a few things to keep in mind when choosing between mean and median to estimate the location of a dataset:\n\nIf a sample has a symmetrical distribution, then the mean and median are equal.\nIf the distribution of a sample is not symmetrical, the mean should not be used.\nThe mean is highly sensitive to outliers, whereas the median is not.\n\n\n\n\n\nListing 2.13: Compute the median for two significant digits.\n\n\nfig, ax = plt.subplots()\nsns.histplot(data=df, x='Zr')\nax.axvline(df['Zr'].mean(), color='darkorange', lw=3, label='Mean')\nax.axvline(df['Zr'].median(), color='darkviolet', lw=3, label='Median')\nax.legend()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDispersion\n\nRange\nThe first information we might want to get for a dataset is the range of values it covers, or range. For this, we need to get the minimum (df.min()) and maximum (df.max()) values, from which, when needed, the range can be calculated with Equation 2.2. It is however likely that the min/max functions will be more frequently used (Listing 2.14).\n\\[\n\\text{Range} = \\max(x) - \\min(x)\n\\tag{2.2}\\]\n\n\n\n\nListing 2.14: Compute the min and the max values of a column\n\n\ndf['Zr'].min()\ndf['Zr'].max()\n\n\n\n\n920.1744058431505\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat is the mean value for Cesium?\n\n\n10.46104.661045.5910455.91\n\n\n\n\n\n\nVariance and standard deviation\nThe variance and standard deviation are two key measures of dispersion that describe how spread out the values in a dataset are.\n\nStandard deviation (\\(\\sigma\\)) measures sum of squares differences between data points \\(x_i\\) and the mean \\(\\bar{x}\\): \\[\n  \\sigma = \\sqrt{\\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1} }\n  \\] where \\(x_i\\) are the data points, \\(\\bar{x}\\) is the mean, and \\(n\\) is the number of observations.\nVariance (\\(\\sigma^2\\)) is the square of the standard deviation: \\[\n  \\sigma^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\n  \\]\n\nFor now we will consider that they roughly measure the same thing. The standard deviation is in the same units as the data, making it easier to interpret.\nRelation to the Gaussian distribution:\nFor a Gaussian (normal) distribution, about 68% of the data falls within one standard deviation of the mean, about 95% within two standard deviations, and about 99.7% within three standard deviations. Thus, variance and standard deviation are fundamental for describing the spread and probability intervals of normally distributed data.\nYou can compute these in pandas using .var() and .std():\n\n\n\nSkewness x\n\n\n\nBivariate statistics x\n\nCorrelation x\n\n\nSimple linear regression x\n\n\n\n\nPetrelli, Maurizio. 2021. Introduction to Python in Earth Science Data Analysis: From Descriptive Statistics to Machine Learning. Springer Textbooks in Earth Sciences, Geography and Environment. Springer Cham. https://doi.org/10.1007/978-3-030-78055-5.\n\n\nSmith, V. C., R. Isaia, and N. J. G. Pearce. 2011. “Tephrostratigraphy and Glass Compositions of Post-15 Kyr Campi Flegrei Eruptions: Implications for Eruption History and Chronostratigraphic Markers.” Quaternary Science Reviews 30 (25-26): 3638–60. https://doi.org/10.1016/j.quascirev.2011.07.012.",
    "crumbs": [
      "Class 1",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data exploration</span>"
    ]
  },
  {
    "objectID": "demo_page.html",
    "href": "demo_page.html",
    "title": "3  Demo page",
    "section": "",
    "text": "3.1 Exercise interactive code with hints and solution",
    "crumbs": [
      "Class 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Demo page</span>"
    ]
  },
  {
    "objectID": "demo_page.html#exercise-interactive-code-with-hints-and-solution",
    "href": "demo_page.html#exercise-interactive-code-with-hints-and-solution",
    "title": "3  Demo page",
    "section": "",
    "text": "ExerciseHintsSolution\n\n\n\n#| setup: true\n#| exercise: ex_4\nlibrary(idarps)\n\n#| exercise: ex_4\ndata(\"pharmacy\")\n# Construct attendance by day\nmonday = na.omit(pharmacy$attendance[pharmacy$weekday == \"Monday\"])\ntuesday = na.omit(pharmacy$attendance[pharmacy$weekday == \"Tuesday\"])\nwednesday = na.omit(pharmacy$attendance[pharmacy$weekday == \"Wednesday\"])\nthursday = na.omit(pharmacy$attendance[pharmacy$weekday == \"Thursday\"])\nfriday = na.omit(pharmacy$attendance[pharmacy$weekday == \"Friday\"])\nsaturday = na.omit(pharmacy$attendance[pharmacy$weekday == \"Saturday\"])\nsunday = na.omit(pharmacy$attendance[pharmacy$weekday == \"Sunday\"])\nidarps::___________(monday, tuesday, wednesday,\n                    thursday, friday, saturday, sunday)\n\n\n\nConsider using the boxplot_w_points() function from package idarps.\n?idarps::boxplot_w_points\n\n\n\n\nSolution. Use the function boxplot_w_points() from package idarps:\ndata(\"pharmacy\")\n\n# Construct attendance by day\nmonday = na.omit(pharmacy$attendance[pharmacy$weekday == \"Monday\"])\ntuesday = na.omit(pharmacy$attendance[pharmacy$weekday == \"Tuesday\"])\nwednesday = na.omit(pharmacy$attendance[pharmacy$weekday == \"Wednesday\"])\nthursday = na.omit(pharmacy$attendance[pharmacy$weekday == \"Thursday\"])\nfriday = na.omit(pharmacy$attendance[pharmacy$weekday == \"Friday\"])\nsaturday = na.omit(pharmacy$attendance[pharmacy$weekday == \"Saturday\"])\nsunday = na.omit(pharmacy$attendance[pharmacy$weekday == \"Sunday\"])\n1idarps::boxplot_w_points(monday, tuesday, wednesday,\n                         thursday, friday, saturday, sunday)           \n                     \n          \n\n1\n\nUse function boxplot_w_points() from the idarps package to create a boxplot with points overlaid.",
    "crumbs": [
      "Class 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Demo page</span>"
    ]
  },
  {
    "objectID": "demo_page.html#python-code-chunk",
    "href": "demo_page.html#python-code-chunk",
    "title": "3  Demo page",
    "section": "3.2 Python code chunk",
    "text": "3.2 Python code chunk\n\n1+1\n\n2",
    "crumbs": [
      "Class 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Demo page</span>"
    ]
  },
  {
    "objectID": "demo_page.html#r-code-chunk",
    "href": "demo_page.html#r-code-chunk",
    "title": "3  Demo page",
    "section": "3.3 R code chunk",
    "text": "3.3 R code chunk\n\n1+1\n\n[1] 2",
    "crumbs": [
      "Class 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Demo page</span>"
    ]
  },
  {
    "objectID": "demo_page.html#iframe",
    "href": "demo_page.html#iframe",
    "title": "3  Demo page",
    "section": "3.4 Iframe",
    "text": "3.4 Iframe",
    "crumbs": [
      "Class 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Demo page</span>"
    ]
  },
  {
    "objectID": "demo_page.html#local-pdf-slides",
    "href": "demo_page.html#local-pdf-slides",
    "title": "3  Demo page",
    "section": "3.5 Local pdf slides",
    "text": "3.5 Local pdf slides",
    "crumbs": [
      "Class 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Demo page</span>"
    ]
  },
  {
    "objectID": "demo_page.html#python-interactive",
    "href": "demo_page.html#python-interactive",
    "title": "3  Demo page",
    "section": "3.6 Python interactive",
    "text": "3.6 Python interactive\n\nfor x in range(5):\n  print(x ** 2)\nimport matplotlib.pyplot as plt\nplt.plot([1, 2, 3, 4], [1, 4, 9, 16])\nplt.ylabel('Magic Numbers')\nplt.show()",
    "crumbs": [
      "Class 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Demo page</span>"
    ]
  },
  {
    "objectID": "demo_page.html#r-interactive",
    "href": "demo_page.html#r-interactive",
    "title": "3  Demo page",
    "section": "3.7 R interactive",
    "text": "3.7 R interactive\n\nlibrary(idarps)\ndata(\"pharmacy\")\n\n# Construct attendance by day\nmonday = na.omit(pharmacy$attendance[pharmacy$weekday == \"Monday\"])\ntuesday = na.omit(pharmacy$attendance[pharmacy$weekday == \"Tuesday\"])\nwednesday = na.omit(pharmacy$attendance[pharmacy$weekday == \"Wednesday\"])\nthursday = na.omit(pharmacy$attendance[pharmacy$weekday == \"Thursday\"])\nfriday = na.omit(pharmacy$attendance[pharmacy$weekday == \"Friday\"])\nsaturday = na.omit(pharmacy$attendance[pharmacy$weekday == \"Saturday\"])\nsunday = na.omit(pharmacy$attendance[pharmacy$weekday == \"Sunday\"])\nidarps::boxplot_w_points(monday, tuesday, wednesday, thursday, friday, saturday, sunday)",
    "crumbs": [
      "Class 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Demo page</span>"
    ]
  },
  {
    "objectID": "demo_page.html#mcq",
    "href": "demo_page.html#mcq",
    "title": "3  Demo page",
    "section": "3.8 MCQ",
    "text": "3.8 MCQ",
    "crumbs": [
      "Class 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Demo page</span>"
    ]
  },
  {
    "objectID": "demo_page.html#question-1",
    "href": "demo_page.html#question-1",
    "title": "3  Demo page",
    "section": "3.9 Question 1",
    "text": "3.9 Question 1\nHere is a question.\n\n \\(\\frac{1}{10}\\) \\(\\frac{3}{10}\\) \\(\\frac{2}{5}\\) \\(\\frac{52}{100}\\)\n\n\n\n\n\n\n\n\n❓ Hint\n\n\n: \n\n\n\n\n\n💡 Explanation\n\n\n:",
    "crumbs": [
      "Class 1",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Demo page</span>"
    ]
  }
]